---
title: "Adventures in Covariance"
author: "Klemens"
date: "May 23, 2018"
output:
  html_document:
    fig_height: 7
    fig_retina: NULL
    code_folding: "hide"
---

# Load packages:
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(MASS)
library(ellipse)
library(rethinking)
set.seed(5)
``` 

# Easy
## 13E1
Add to the following model varying slopes on the predictor x.
$$
y_i \sim Normal(\mu_i, \sigma)\\
\mu_i = \alpha_{GROUP[i]} + {\beta}x_i\\
\alpha_{GROUP} \sim Normal(\alpha, \sigma_{\alpha})\\
\alpha \sim Normal(0, 10)\\
\beta \sim  Normal(0, 1)\\
\sigma \sim HalfCauchy(0, 2)\\
\sigma_{alpha} \sim HalfCauchy(0, 2)
$$

Answer: 

$$
y_i \sim Normal(\mu_i, \sigma)\\
\mu_i = \alpha_{GROUP[i]} + {\beta_{GROUP[i]}}x_i\\
{
\begin{bmatrix}
\alpha_{GROUP} \\
\beta_{GROUP}
\end{bmatrix}
}
\sim Normal\left(\begin{bmatrix}
\alpha \\
\beta
\end{bmatrix}
, S \right)\\
S =  
\begin{pmatrix}
      \sigma_{\alpha} & 0 \\
      0 & \sigma_{\beta}
\end{pmatrix}
R
\begin{pmatrix}
      \sigma_{\alpha} & 0 \\
      0 & \sigma_{\beta}
\end{pmatrix}\\
\alpha \sim Normal(0, 10)\\
\beta \sim  Normal(0, 1)\\
\sigma \sim HalfCauchy(0, 2)\\
\sigma_{\alpha} \sim HalfCauchy(0, 2)\\
\sigma_{\beta} \sim HalfCauchy(0, 2)\\
R \sim LKJcorr(2)
$$



## 13E2
Think up a context in which varying intercepts will be positively correlated with varying slopes.
Provide a mechanistic explanation for the correlation.

Answer: 
Think about the cafes example. Now suppose that cafes with a long waiting time in the morning have a even longer (short) waiting time in the evening and vice verca. This will result in a posetive (negativ) correlation.
The correlation measures the strengh and direction of a linear conection between X and Y.

$\rho_{X, Y} = corr(X, Y) = \frac{cov(X, Y)}{\sigma_X\sigma_Y}$


## 13E3
When is it possible for a varying slopes model to have fewer effective parameters (as estimated by WAIC or DIC) than the corresponding model with fixed (unpooled) slopes?

Answer:
If the prior assigned to each intercept shrinks them all towards the mean, this will result in fewer effective parameters. 

If we have an aggressive regularizing prior, this will result in a less flexible posterior and therefore fewer effective parameters. (p. 360)

See also p. 407:
"The [1] index in each vector is the varying intercept standard deviation, while the [2] and [3] are the slopes. While these are just posterior means, and the amount of shrinkage averages over the entire posterior, you can get a sense from the small values that shrinkage is pretty aggressive here. This is what takes the model from 56 actual parameters to 18 effective parameters, as measured by WAIC."


# Medium:
## 13M1
Repeat the café robot simulation from the beginning of the chapter. This time, set rho to zero,
so that there is no correlation between intercepts and slopes. How does the posterior distribution of
the correlation reflect this change in the underlying simulation?

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# define the entire population of cafes:
a <- 3.5        # average morning wait time
b <- (-1)       # average difference afternoon wait time
sigma_a <- 1    # std dev in intercepts
sigma_b <- 0.5   # std dev in slopes
# <- (-0.7)    # correlation between intercepts and slopes
rho <- 0

Mu <- c(a, b)   # vector of two means
sigmas <- c(sigma_a, sigma_b)
Rho <- matrix(c(1, rho, rho, 1), nrow = 2)

# now matrix multiply to get covariance matrix
Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)
N_cafes <- 20   # number of cafes (shops) you simulated
library(MASS)
set.seed(5)
vary_effects <- mvrnorm(N_cafes, Mu, Sigma)

a_cafe <- vary_effects[,1]
b_cafe <- vary_effects[,2]

plot(a_cafe, b_cafe, col=rethinking::rangi2,
     xlab = "intercept (a_cafe)",
     ylab = "slopes (b_cafe)")

# overlay population distribution
for (l in c(0.1,0.3,0.5,0.8,0.99))
     lines(ellipse(Sigma,centre=Mu,level=l),
           col=rethinking::col.alpha("black",0.2))

# 13.1.2. Simulate observations:
N_visits <- 10
afternoon <- rep(0:1, N_visits*N_cafes/2)
cafe_id <- rep(1:N_cafes, each = N_visits)

mu <- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon
sigma <- .5   # std dev within cafes
wait <- rnorm(N_visits*N_cafes, mu, sigma)
d <- data.frame(cafe=cafe_id, afternoon=afternoon, wait=wait)


R <- rlkjcorr(1e4, K=2, eta=2)
dens(R[,1,2], xlab="correlation")

# Model:
m13M1 <- map2stan(
  alist(
    wait <- dnorm(mu, sigma),
    mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,
    c(a_cafe, b_cafe)[cafe] ~ dmvnorm2(c(a, b), sigma_cafe, Rho),
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    sigma_cafe ~ dcauchy(0, 2),
    sigma ~ dcauchy(0, 2),
    Rho ~ dlkjcorr(2)
  ),
  data = d,
  iter = 5000, warmup = 2000, chains = 2
)

post <- extract.samples(m13M1)
dens(post$Rho[,1,2])
```

The posterior shifts to the right with a peak at roughly 0.5. The prior might no longer learn from a negative correlation but rather from a slightly positive one. 


## 13M2
Fit this multilevel model to the simulated café data:
$$
Wi \sim Normal(\mu_i, \sigma)\\
\mu_i = \alpha_{café[i]} + \beta_{café[i]}Ai\\
\alpha_{café[i]} \sim Normal(\alpha, \sigma_\alpha)\\
\beta_{café[i]} \sim Normal(\beta, \sigma_\beta)\\
\alpha \sim Normal(0, 10)\\
\beta \sim Normal(0, 10)\\
 \sigma \sim HalfCauchy(0, 1)\\
\sigma_\alpha \sim HalfCauchy(0, 1)\\
\sigma_\beta \sim HalfCauchy(0, 1)\\
$$
Use WAIC to compare this model to the model from the chapter, the one that uses a multi-variate
Gaussian prior. Explain the result.


```{r, message=FALSE, warning=FALSE}
set.seed(5)
rho <- -.7
#rho <- 0

Mu <- c(a, b)   # vector of two means
sigmas <- c(sigma_a, sigma_b)
Rho <- matrix(c(1, rho, rho, 1), nrow = 2)

# now matrix multiply to get covariance matrix
Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)
N_cafes <- 20   # number of cafes (shops) you simulated
vary_effects <- mvrnorm(N_cafes, Mu, Sigma)

a_cafe <- vary_effects[,1]
b_cafe <- vary_effects[,2]

plot(a_cafe, b_cafe, col=rethinking::rangi2,
     xlab = "intercept (a_cafe)",
     ylab = "slopes (b_cafe)")

# overlay population distribution
for (l in c(0.1,0.3,0.5,0.8,0.99))
     lines(ellipse(Sigma,centre=Mu,level=l),
           col=rethinking::col.alpha("black",0.2))

# 13.1.2. Simulate observations:
N_visits <- 10
afternoon <- rep(0:1, N_visits*N_cafes/2)
cafe_id <- rep(1:N_cafes, each = N_visits)

mu <- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon
sigma <- .5   # std dev within cafes
wait <- rnorm(N_visits*N_cafes, mu, sigma)
d <- data.frame(cafe=cafe_id, afternoon=afternoon, wait=wait)


R <- rlkjcorr(1e4, K=2, eta=2)
dens(R[,1,2], xlab="correlation")

# Model:
m13M1 <- map2stan(
  alist(
    wait <- dnorm(mu, sigma),
    mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,
    c(a_cafe, b_cafe)[cafe] ~ dmvnorm2(c(a, b), sigma_cafe, Rho),
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    sigma_cafe ~ dcauchy(0, 2),
    sigma ~ dcauchy(0, 2),
    Rho ~ dlkjcorr(2)
  ),
  data = d,
  iter = 5000, warmup = 2000, chains = 2
)

post <- extract.samples(m13M1)
dens(post$Rho[,1,2])

###############################################################################
m13M2 <- map2stan(
  alist(
    wait ~ dnorm(mu , sigma ),
    mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,
    a_cafe[cafe] ~ dnorm(a, sigma_a),
    b_cafe[cafe] ~ dnorm(b, sigma_b),
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 1),
    sigma_a ~ dcauchy(0, 1),
    sigma_b ~ dcauchy(0, 1)
  ),
  data=d ,
  iter=5000 , warmup=2000 , chains=2 )

# compare models
compare(m13M1, m13M2)
posterior.samples.m13M1 <- extract.samples(m13M1)
a13M1 <- apply( X = posterior.samples.m13M1$a_cafe , MARGIN = 2 , FUN = mean )
b13M1 <- apply( X = posterior.samples.m13M1$b_cafe , MARGIN = 2 , FUN = mean )
posterior.samples.m13M2 <- extract.samples(m13M2)
a13M2 <- apply( X = posterior.samples.m13M2$a_cafe, MARGIN = 2 , FUN = mean )
b13M2 <- apply( X = posterior.samples.m13M2$b_cafe, MARGIN = 2 , FUN = mean )

plot( a13M1 , b13M1 , xlab="intercept" , ylab="slope" ,
      pch=16 , col=rangi2 , ylim=c( min(b13M1)-0.05 , max(b13M1)+0.05 ) ,
      xlim=c( min(a13M1)-0.1 , max(a13M1)+0.1 ) )
points( a13M2 , b13M2 , pch=1 )

```

The WAIC is pretty much the same between both models, slightly prefering m13M1. 
The data-generation process included a negative correlation between intercept
and slope. Because the intercepts (x-values) are larger than average to the
right of the center, the blue points (the ones from the varying slopes model 13M1 "including
correlation") are pushed to be smaller than average on the y-axis when comparing them to the points from model 13M2. Conversely, the x-values to the left of center push the y-values in blue to be larger.


## 13M3
Re-estimate the varying slopes model for the UCBadmit data, now using a non-centered parameterization.
Compare the efficiency of the forms of the model, using n_eff. Which is better?
Which chain sampled faster?
```{r, message=FALSE, warning=FALSE}
data(UCBadmit)
d <- UCBadmit
d$male <- ifelse( d$applicant.gender=="male" , 1 , 0 )
d$dept_id <- coerce_index( d$dept )
m13M3 <- map2stan(
  alist(
    admit ~ dbinom( applications, p ),
    logit(p) <- a_dept[dept_id] +
                bm_dept[dept_id]*male,
    c(a_dept, bm_dept)[dept_id] ~ dmvnorm2( c(a, bm), sigma_dept, Rho ),
    a ~ dnorm(0, 10),
    bm ~ dnorm(0, 1),
    sigma_dept ~ dcauchy(0, 2),
    Rho ~ dlkjcorr(2)
  ),
  data=d , warmup=1000 , iter=5000 , chains=4 , cores=3 )

m13M3.noncentered <- map2stan(
  alist(
    admit ~ dbinom( applications, p ),
    logit(p) <- a_dept[dept_id] + bm_dept[dept_id]*male,
    c(a_dept, bm_dept)[dept_id] ~ dmvnormNC( sigma_dept, Rho ),
    a ~ dnorm(0, 10),
    bm ~ dnorm(0, 1),
    sigma_dept ~ dcauchy(0, 2),
    Rho ~ dlkjcorr(2)
  ),
  data=d , warmup=1000 , iter=5000 , chains=4 , cores=3 )

# compare centered and non-centered models
compare(m13M3, m13M3.noncentered)

# extract n_eff values for each model
neff_c <- precis(m13M3,2)@output$n_eff
neff_nc <- precis(m13M3.noncentered,2)@output$n_eff
# plot distributions
boxplot( list( 'm13M3'=neff_c , 'm13M3.noncentered'=neff_nc ) , ylab="effective samples" , xlab="model" )



```

Models look identical, with the same "flexibility" (WAIC), similar weight.
This said, the non-centered model samples much more efficiently.


# Hard:
## 13H1
Let’s revisit the Bangladesh fertility data, data(bangladesh), from the practice problems for Chapter 12. Fit a model with both varying intercepts by district_id and varying slopes of urban by district_id. You are still predicting use.contraception. Inspect the correlation between the intercepts and slopes. Can you interpret this correlation, in terms of what it tells you about the pattern of contraceptive use in the sample? It might help to plot the mean (or median) varying effect estimates for both the intercepts and slopes, by district. Then you can visualize the correlation and maybe more easily think through what it means to have a particular correlation. Plotting predicted proportion of women using contraception, with urban women on one axis and rural on the other, might also help.

```{r, message=FALSE, warning=FALSE}
data(bangladesh)
d <- bangladesh
d$district_id <- coerce_index(d$district) # in district #54 is missing
d$use_contraception <- d$use.contraception 

m13H1 <- map2stan(
  alist(
    use_contraception ~ dbinom(1, p),
    #model
    logit(p) <- alpha + alpha_district[district_id] + (beta + beta_district[district_id])*urban,
    # adaptive prior
    c(alpha_district, beta_district)[district_id] ~ dmvnorm2(Mu = 0, sigma = Sigma, Rho = Rho),
    # fixed prior
    alpha ~ dnorm(0, 10),
    beta ~ dnorm(0, 10),
    Sigma ~ dcauchy(0, 2),
    Rho ~ dlkjcorr(2)
  ),
  data = d, chains = 2, warmup = 1000, iter = 4000
)

# inspect posterior distribution for Rho
posterior.samples <- extract.samples(m13H1)
dens( posterior.samples$Rho[,1,2] )

# inspect estimates
precis(m13H1, pars = c("alpha", "beta"), depth = 2)
```

Urban context is of comparatively smaller impact on contraceptive use in districts (within-cluster) that have higher-than-average contraceptive use (in rural areas); urban context is of comparatively higher impact on contraceptive use in districts that have lower-than-average contraceptive use (in rural areas).
(Cofe example)?

```{r, message=FALSE, warning=FALSE}
# take some of the author's plots
pred.dat.rural <- list(
  urban=rep(0, 60),
  district_id=1:60 )

pred.dat.urban <- list(
  urban=rep(1, 60),
  district_id=1:60 )

# predict:
pred.rural <- link(m13H1, data=pred.dat.rural)
pred.urban <- link(m13H1, data=pred.dat.urban)

means.rural <- apply(pred.rural, 2, mean)
means.urban <- apply(pred.urban, 2, mean)

plot(means.rural, means.urban , col="slateblue" ,
     xlim=c(0,1) , ylim=c(0,1) ,
     xlab="rural use" , ylab="urban use" )
     abline(a=0,b=1,lty=2)

plot(means.rural , means.urban-means.rural , col="slateblue" ,
     xlab="rural use" , ylab="difference btw urban and rural" )
     abline(h=0,lty=2)
```


## 13H2
Varying effects models are useful for modeling time series, as well as spatial clustering. In a time series, the observations cluster by entities that have continuity through time, such as individuals.
Since observations within individuals are likely highly correlated, the multilevel structure can help quite a lot. You’ll use the data in data(Oxboys), which is 234 height measurements on 26 boys from an Oxford Boys Club (I think these were like youth athletic leagues?), at 9 different ages (centered and standardized) per boy. You’ll be interested in predicting height, using age, clustered by Subject (individual boy).
Fit a model with varying intercepts and slopes (on age), clustered by Subject. Present and interpret the parameter estimates. Which varying effect contributes more variation to the heights, the intercept or the slope?

```{r, message=FALSE, warning=FALSE}
data(Oxboys)
d <- Oxboys
d$height_centered <- (d$height - mean(d$height)) / sd(d$height)

m13H2 <- map2stan(
  alist(
    height_centered ~ dnorm(mu, sigma),
    # model
    mu <- alpha + alpha_subject[Subject] + (beta + beta_subject[Subject])*age,
    alpha ~ dnorm(0, 10),
    beta ~ dnorm(0, 10),
    # adaptive prior
    c(alpha_subject, beta_subject)[Subject] ~ dmvnormNC(sigma = sigma_subject, Rho = Rho),
    # fixed prior
    sigma ~ dcauchy(0, 2),
    sigma_subject ~ dcauchy(0, 2),
    Rho ~ dlkjcorr(2)
  ),
  data = d, chains = 2, warmup = 1000, iter = 4000
)

# inspect posterior distribution for Rho
posterior.samples <- extract.samples(m13H2)
dens(posterior.samples$Rho[,1,2])

# inspect estimates
precis(m13H2, pars = c("alpha", "beta", "alpha_subject"), depth = 2)


```

The intercept because the StdDev is higher.


## 13H3
Now consider the correlation between the varying intercepts and slopes. Can you explain its value? How would this estimated correlation influence your predictions about a new sample of boys?

Answer: Boys that are already taller than most grow faster. Therefore in a new sample I would assume that taller boys grow faster. Or my prior wouldn´t be to conservative on a new data set where tall boys are growing faster.


## 13H4
Use mvrnorm (in library(MASS)) or rmvnorm (in library(mvtnorm)) to simulate a new
sample of boys, based upon the posterior mean values of the parameters. That is, try to simulate varying intercepts and slopes, using the relevant parameter estimates, and then plot the predicted trends of height on age, one trend for each simulated boy you produce. A sample of 10 simulated boys is plenty, to illustrate the lesson. You can ignore uncertainty in the posterior, just to make the problem a little easier. But if you want to include the uncertainty about the parameters, go for it.
Note that you can construct an arbitrary variance-covariance matrix to pass to either mvrnorm or rmvnorm with something like:

```{r, eval=FALSE, message=FALSE, warning=FALSE, include=T, echo = TRUE}
S <- matrix( c( sa^2 , sa*sb*rho , sa*sb*rho , sb^2 ) , nrow=2 )
```

where sa is the standard deviation of the first variable, sb is the standard deviation of the second variable, and rho is the correlation between them.
